{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jj9jpF54iery"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import argparse\n",
    "from random import random\n",
    "\n",
    "from torch import optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XP8UGMq1idND"
   },
   "outputs": [],
   "source": [
    "def preprocess(au_mfcc_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(au_mfcc_path, 'rb') as f:\n",
    "        au_mfcc = pickle.load(f)\n",
    "\n",
    "    print(len(au_mfcc))\n",
    "\n",
    "    for key in au_mfcc:\n",
    "        emotion = key.split('-')[2]   #这边是要看au_mfcc里面的样子\n",
    "        emotion = int(emotion)-1\n",
    "        labels.append(emotion)\n",
    "        data.append(au_mfcc[key])\n",
    "\n",
    "\n",
    "    # 将data和labels转换为numpy数组，方便后续操作\n",
    "    data=np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    # 调整labels的形状，增加一个维度，使其成为二维数组\n",
    "    labels = labels.reshape(labels.shape+(1,))\n",
    "\n",
    "   # 将data和labels在水平方向上拼接，形成一个新的数组\n",
    "    data = np.hstack((data, labels))\n",
    "    # 对拼接后的数据进行随机打乱，保证数据的随机性\n",
    "    fdata = shuffle(data)\n",
    "\n",
    "    # 将打乱后的数据分为特征数据和标签数据\n",
    "    data = fdata[:, :-1]  # 取所有行，除了最后一列（标签）\n",
    "    labels = fdata[:, -1].astype(int)  # 取所有行的最后一列，并转换为整数类型\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_x7tP9HJO6ZJ"
   },
   "outputs": [],
   "source": [
    "class MMF_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MMF_Model, self).__init__()\n",
    "\n",
    "        rnn = nn.LSTM\n",
    "\n",
    "        self.au_rnn1 = rnn(35, 16, bidirectional=True) #输入维度为35（AU特征的维度），隐藏层维度为16，bidirectional=True双向LSTM。\n",
    "        self.au_rnn2 = rnn(2*16, 16, bidirectional=True)\n",
    "        #输入维度为前一层的两倍（因为是双向LSTM，所以输出维度是隐藏层维度的两倍），隐藏层维度为16，双向LSTM。\n",
    "\n",
    "    # 第一层LSTM的输出维度是 2*16（因为是双向LSTM）。\n",
    "    # 第二层LSTM的输出维度也是 2*16（因为是双向LSTM）。\n",
    "    # 将这两层的输出拼接在一起，得到的维度是 2*16 + 2*16 = 64。\n",
    "\n",
    "        self.mfccs_rnn1 = rnn(259, 16, bidirectional=True)\n",
    "        self.mfccs_rnn2 = rnn(2*16, 16, bidirectional=True)\n",
    "\n",
    "        self.fusion_layer = nn.Linear(in_features=128, out_features=8)\n",
    "    #这边的128是64+64 两个双向LSTM层合并\n",
    "\n",
    "    def extract_au(self, au, lengths):\n",
    "        packed_sequence = pack_padded_sequence(au, lengths)\n",
    "        packed_h1, (final_h1, _) = self.au_rnn1(packed_sequence)\n",
    "    # self.au_rnn1(packed_sequence)会返回一个元组，包含两个元素：\n",
    "    # packed_h1：这是LSTM层的输出，即经过LSTM处理后的序列数据，仍然是一个PackedSequence对象。\n",
    "    # (final_h1, _)： 这是LSTM层的最终隐藏状态和细胞状态。对于双向LSTM，隐藏状态会有两个部分（正向和反向），\n",
    "    # 所以形状是(2, batch_size, hidden_size)。这里只关心隐藏状态final_h1，而细胞状态被忽略（用_表示）。\n",
    "        \n",
    "        padded_h1, _ = pad_packed_sequence(packed_h1)  #这里将第一层LSTM的输出 packed_h1 解包成普通的张量 padded_h1，以便进行后续操作。\n",
    "        packed_normed_h1 = pack_padded_sequence(padded_h1, lengths) \n",
    "        _, (final_h2, _) = self.au_rnn2(packed_normed_h1)\n",
    "    #packed_h1, (final_h1, _) = self.au_rnn1(packed_sequence) 与  _, (final_h2, _) = self.au_rnn2(packed_normed_h1)的区别\n",
    "    # 第一层LSTM的输出序列需要进一步处理：\n",
    "    # 在第一层LSTM中，输出序列packed_h1需要被解包成普通的张量，以便进行后续的处理，如再次包装成PackedSequence对象并输入到第二层LSTM中。\n",
    "    # 因此，第一行代码中保留了packed_h1。\n",
    "    # 第二层LSTM的输出序列不需要进一步处理：\n",
    "    # 在第二层LSTM中，我们只关心最终的隐藏状态final_h2，因为这些隐藏状态将被用于提取高层次的特征表示。\n",
    "    # 因此，第二行代码中忽略了输出序列，只保留了final_h2\n",
    "        extracted_au = torch.cat((final_h1, final_h2), dim=2).permute(1,0,2).contiguous().view(batch_size,-1)\n",
    "    # torch.cat((final_h1, final_h2), dim=2)：将两层LSTM的最终隐藏状态在最后一个维度上拼接，得到形状为 (2, batch_size, 32) 的张量。\n",
    "    # .permute(1, 0, 2)：调整维度顺序，得到形状为 (batch_size, 2, 32) 的张量。\n",
    "    # .contiguous()：确保张量在内存中是连续的。\n",
    "    # .view(batch_size, -1)：将张量展平为二维，得到形状为 (batch_size, 64) 的张量。\n",
    "        return extracted_au\n",
    "\n",
    "    def extract_mfccs(self, mfccs, lengths):\n",
    "\n",
    "        packed_sequence = pack_padded_sequence(mfccs, lengths)\n",
    "        packed_h1, (final_h1, _) = self.mfccs_rnn1(packed_sequence)\n",
    "        padded_h1, _ = pad_packed_sequence(packed_h1)\n",
    "        packed_normed_h1 = pack_padded_sequence(padded_h1, lengths)\n",
    "        _, (final_h2, _) = self.mfccs_rnn2(packed_normed_h1)\n",
    "        extracted_mfccs = torch.cat((final_h1, final_h2), dim=2).permute(1,0,2).contiguous().view(batch_size,-1)\n",
    "\n",
    "        return extracted_mfccs\n",
    "\n",
    "    def forward(self, au, mfccs, lengths):\n",
    "        batch_size = 60\n",
    "\n",
    "        extracted_au = self.extract_au(au, lengths)\n",
    "        extracted_mfccs = self.extract_mfccs(mfccs, lengths)\n",
    "\n",
    "        au_mfccs_fusion = torch.cat((extracted_au, extracted_mfccs), dim=1)\n",
    "\n",
    "        final_output = self.fusion_layer(au_mfccs_fusion)\n",
    "        return final_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data, labels, mode=None, to_print=False):\n",
    "    assert(mode is not None)\n",
    "\n",
    "    model.eval() #评估模式\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    eval_loss, eval_loss_diff = [], [] #存储评估损失率\n",
    "\n",
    "    if mode == \"test\":\n",
    "        if to_print:\n",
    "            model.load_state_dict(torch.load(\n",
    "                f'C:/Users/yangc/deep_learning group_asg/fusion_model.ckpt'))#如果mode是\"test\"且to_print为True，则加载预训练的模型权重\n",
    "\n",
    "    corr=0\n",
    "    with torch.no_grad():                     #在评估过程中不需要计算梯度，因此可以禁用梯度计算以提高效率。\n",
    "        for i in range(0, len(data), 60):\n",
    "            model.zero_grad()\n",
    "            # v, a, y, l = batch\n",
    "            d=data[i:i+60]\n",
    "            l=labels[i:i+60]\n",
    "            d=np.expand_dims(d,axis=0)\n",
    "            au=torch.from_numpy(d[:, :, :35]).float()\n",
    "            mfccs=torch.from_numpy(d[:, :, 35:]).float()\n",
    "            y=torch.from_numpy(l).float()\n",
    "\n",
    "            lengths = torch.LongTensor([au.shape[0]]*au.size(1))\n",
    "\n",
    "            au = au.cuda()\n",
    "            mfccs = mfccs.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            output = model(au, mfccs, lengths)\n",
    "\n",
    "            loss =  criterion(output, y)\n",
    "\n",
    "            eval_loss.append(loss.item())\n",
    "            preds=output.detach().cpu().numpy()\n",
    "            y_trues=y.detach().cpu().numpy()\n",
    "\n",
    "            for j in range(len(preds)):\n",
    "                pred=np.argmax(preds[j])\n",
    "                y_true=np.argmax(y_trues[j])\n",
    "                if pred==y_true:\n",
    "                    corr+=1\n",
    "\n",
    "    eval_loss = np.mean(eval_loss)\n",
    "\n",
    "    accuracy = corr/(1.0*len(labels))\n",
    "\n",
    "    return eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2cz6zB2I42R",
    "outputId": "33c81438-bcaa-493d-dbf5-4cad3503f89c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yangc\\AppData\\Local\\Temp\\ipykernel_30880\\626804640.py:5: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  au_mfcc = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n",
      "u: 8\n",
      "=====Epoch1======\n",
      "train_loss: 2.024, train_acc: 27.06%\n",
      "valid_loss: 2.023, valid_acc: 26.67%\n",
      "=====Epoch2======\n",
      "train_loss: 1.976, train_acc: 30.20%\n",
      "valid_loss: 1.977, valid_acc: 29.17%\n",
      "=====Epoch3======\n",
      "train_loss: 1.884, train_acc: 40.39%\n",
      "valid_loss: 1.878, valid_acc: 42.08%\n",
      "=====Epoch4======\n",
      "train_loss: 1.745, train_acc: 49.12%\n",
      "valid_loss: 1.737, valid_acc: 50.83%\n",
      "=====Epoch5======\n",
      "train_loss: 1.591, train_acc: 51.76%\n",
      "valid_loss: 1.577, valid_acc: 51.67%\n",
      "=====Epoch6======\n",
      "train_loss: 1.452, train_acc: 51.76%\n",
      "valid_loss: 1.432, valid_acc: 52.50%\n",
      "=====Epoch7======\n",
      "train_loss: 1.348, train_acc: 54.90%\n",
      "valid_loss: 1.309, valid_acc: 58.33%\n",
      "=====Epoch8======\n",
      "train_loss: 1.266, train_acc: 57.45%\n",
      "valid_loss: 1.220, valid_acc: 58.75%\n",
      "=====Epoch9======\n",
      "train_loss: 1.193, train_acc: 58.92%\n",
      "valid_loss: 1.160, valid_acc: 60.00%\n",
      "=====Epoch10======\n",
      "train_loss: 1.143, train_acc: 58.73%\n",
      "valid_loss: 1.114, valid_acc: 56.67%\n",
      "=====Epoch11======\n",
      "train_loss: 1.086, train_acc: 61.76%\n",
      "valid_loss: 1.058, valid_acc: 60.00%\n",
      "=====Epoch12======\n",
      "train_loss: 1.044, train_acc: 62.35%\n",
      "valid_loss: 1.022, valid_acc: 61.67%\n",
      "=====Epoch13======\n",
      "train_loss: 1.006, train_acc: 64.02%\n",
      "valid_loss: 0.986, valid_acc: 63.33%\n",
      "=====Epoch14======\n",
      "train_loss: 0.979, train_acc: 63.92%\n",
      "valid_loss: 0.954, valid_acc: 64.58%\n",
      "=====Epoch15======\n",
      "train_loss: 0.934, train_acc: 65.78%\n",
      "valid_loss: 0.924, valid_acc: 67.08%\n",
      "=====Epoch16======\n",
      "train_loss: 0.911, train_acc: 66.67%\n",
      "valid_loss: 0.902, valid_acc: 67.92%\n",
      "=====Epoch17======\n",
      "train_loss: 0.880, train_acc: 66.86%\n",
      "valid_loss: 0.891, valid_acc: 70.42%\n",
      "=====Epoch18======\n",
      "train_loss: 0.865, train_acc: 67.35%\n",
      "valid_loss: 0.872, valid_acc: 70.00%\n",
      "=====Epoch19======\n",
      "train_loss: 0.835, train_acc: 68.63%\n",
      "valid_loss: 0.850, valid_acc: 72.50%\n",
      "=====Epoch20======\n",
      "train_loss: 0.809, train_acc: 70.00%\n",
      "valid_loss: 0.823, valid_acc: 72.08%\n",
      "=====Epoch21======\n",
      "train_loss: 0.795, train_acc: 70.29%\n",
      "valid_loss: 0.817, valid_acc: 72.50%\n",
      "=====Epoch22======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch23======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch24======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch25======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch26======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch27======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch28======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch29======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch30======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch31======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch32======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch33======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch34======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch35======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch36======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch37======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch38======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch39======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch40======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch41======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch42======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch43======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch44======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch45======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch46======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch47======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch48======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch49======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "=====Epoch50======\n",
      "train_loss: 0.795, train_acc: 70.10%\n",
      "valid_loss: 0.840, valid_acc: 68.33%\n",
      "test_loss: 0.804 test_acc: 70.56%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    device = torch.cuda.is_available()\n",
    "\n",
    "    data_path = 'C:/Users/yangc/deep_learning group_asg/au_mfcc.pkl'\n",
    "\n",
    "    data, labels=preprocess(data_path)\n",
    "    print('u:', np.unique(labels.astype(int)).size)\n",
    "    #np.unique(labels.astype(int)).size：计算标签中唯一类别的数量，即分类任务的类别数。\n",
    "    \n",
    "    \n",
    "    new_labels= np.zeros((labels.shape[0], np.unique(labels.astype(int)).size))\n",
    "    for i in range(len(labels)):\n",
    "        new_labels[i, labels[i]]=1\n",
    "        \n",
    "    labels=new_labels\n",
    "    # 将标签转换为one-hot编码\n",
    "\n",
    "    test_data=data[-181:-1] #从原始数据中取出从倒数第181个样本到倒数第2个样本（不包括最后一个样本）作为测试集。\n",
    "    test_labels=labels[-181:-1]\n",
    "    data=data[:-180] #去掉最后的180个样本\n",
    "    labels=labels[:-180]\n",
    "\n",
    "    train_data=data[:1020]  #取data里面的前1020作为训练集\n",
    "    train_labels=labels[:1020]\n",
    "\n",
    "    dev_data=data[1020:] #验证集\n",
    "    dev_labels=labels[1020:]\n",
    "\n",
    "    model = MMF_Model()\n",
    "\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    # optim.Adam：Adam是一种常用的优化算法，结合了AdaGrad和RMSProp的优点，能够自适应地调整学习率。\n",
    "    # filter(lambda p: p.requires_grad, model.parameters())：过滤掉不需要梯度的参数，只优化那些requires_grad=True的参数。\n",
    "    # model.parameters()：返回模型中所有可学习的参数。\n",
    "    # filter(lambda p: p.requires_grad, ...)：筛选出需要梯度的参数。\n",
    "    # lr=1e-3：设置学习率为0.001。\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #这行代码创建了一个交叉熵损失函数的对象 criterion。\n",
    "    #在训练过程中，你会将模型的输出和真实标签传给这个损失函数，计算损失值：\n",
    "\n",
    "    batch_size=60\n",
    "    n_total=len(train_data)\n",
    "    best_loss=float('inf')\n",
    "    for e in range(50):\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        cnt=0\n",
    "        print(f\"=====Epoch{e+1}======\")\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            data=train_data[i:i+60]\n",
    "            label=train_labels[i:i+60]  #    data：当前批次的训练数据。label：当前批次的训练标签。\n",
    "\n",
    "            model.zero_grad()\n",
    "            # v, a, y, l = batch\n",
    "            data=np.expand_dims(data,axis=0)\n",
    "            au=torch.from_numpy(data[:, :, :35]).float()   #将前35个特征分给au\n",
    "            mfccs=torch.from_numpy(data[:, :, 35:]).float() #将后35个特征分给mfccs\n",
    "            y=torch.from_numpy(label).float()\n",
    "\n",
    "            au = au.cuda()\n",
    "            mfccs = mfccs.cuda()\n",
    "            y = y.cuda()\n",
    "            \n",
    "            #模型调优\n",
    "            lengths = torch.LongTensor([au.shape[0]]*au.size(1))\n",
    "            fused_features = model(au, mfccs, lengths)  #模型的输出          模型在这里就结束了\n",
    "            loss = criterion(fused_features, y) #计算模型输出与真实标签之间的损失\n",
    "            loss.backward() # 计算梯度\n",
    "            optimizer.step() # 更新模型参数\n",
    "\n",
    "        train_loss, train_acc = eval(train_data, train_labels, mode=\"train\")\n",
    "        print('train_loss: {:.3f}, train_acc: {:.2f}%'.format(train_loss, 100*train_acc))\n",
    "\n",
    "        valid_loss, valid_acc = eval(dev_data, dev_labels, mode=\"dev\")\n",
    "        print('valid_loss: {:.3f}, valid_acc: {:.2f}%'.format(valid_loss, 100*valid_acc))\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'C:/Users/yangc/deep_learning group_asg/fusion_model.ckpt')\n",
    "            torch.save(optimizer.state_dict(), 'C:/Users/yangc/deep_learning group_asg/optim_best.std')\n",
    "        else:\n",
    "            model.load_state_dict(torch.load('C:/Users/yangc/deep_learning group_asg/fusion_model.ckpt'))\n",
    "            optimizer.load_state_dict(torch.load('C:/Users/yangc/deep_learning group_asg/optim_best.std'))\n",
    "\n",
    "    test_loss, test_acc=eval(test_data, test_labels, mode=\"test\", to_print=True)\n",
    "    print('test_loss: {:.3f} test_acc: {:.2f}%'.format(test_loss, 100*test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwpamMIBdryE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
